---
title: "Machine Learning Project"
output:
  html_document:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---

===============================
load caret and ggplot2

load data



## Timestamps
classe is correlated with time:


will want to strip irrelevant info, user, time, window stuff from training data.


## tidying
write a function to do this, as we will need to tidy the test data also
cvtd timestamp -> to date
 $ max_roll_belt 
  max_picth_belt 
  min_roll_belt   
  min_pitch_belt
amplitude_roll_belt   
A bunch of others

set things to missing, impute the missing values

Then procede




## some exploratory plots

# glm

We'll try a glm (binomial) model (logit). Maybe this does a bunch of models?
make sure we do the preprocessing within the `train` statement, or else it messes up cross validation (We should split the data into folds and conduct PCA within each fold, rather than transforming via PCA and then splitting). 




## basic tree
<!-- library(devtools) -->
<!-- install_bitbucket("mkuhn/parallelRandomForest", ref="parallelRandomForest") -->
## random forest
Important variables


A second random forest, this time using repeated cross validation, and maybe some more variable samplings

ranger2 stuff


ranger3, random forest with pca

First, preprocess the training set to get an idea of how many components to use. We will want to do the actual pca preprocessing within the train method, but want to know how many variables will be present (specify pcaComps). This is needed for the mtry grid used by train (can try a random sample of 40 variables if there are only 12 after pca). 

will use 25 components (for now)


ranger3 stuff

## Actually PCA
use some PCA preprocessing, maybe with another random forest

also look into svm

maybe one other model (gbm?) boosted decision tree?







