---
title: "Activity classification from wearable device measurements"
output:
  html_document:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---

===============================

# Introduction

Data comes from XX. Aim is to predict whether subject is performing a weight lifting excercise correctly, or making one of several common mistakes.

Data will first be partitioned and tidied

Exploratory
 - correlation in time
 - correlation heatmap (after tidying)

 preprocessing - get an idea of how many components to use in PCA 
 should we use pca?


# Data Partitioning

First load required R libraries and set chunk options

load caret and ggplot2

```{r packages, cache=FALSE,message=FALSE,warning=FALSE}
library(knitr)
opts_chunk$set(fig.width=6, fig.height=6,dpi=108)
library(ggplot2)
library(dplyr)
library(caret)
library(ranger)
library(MASS)
library(reshape2)
library(GGally)
```
Load data. 

```{r loaddata,cache=TRUE}
training<-read.csv('./data/pml-training.csv',stringsAsFactors=TRUE,na.strings = c("#DIV/0!","NA"))
dim(training)
# convert cvtd_timestamp to posixlt class
training$cvtd_timestamp<-strptime(training$cvtd_timestamp,format = "%d/%m/%Y %H:%M")
```

The activity classification is specified in the `classe` variable. In the training dataset, this is highly correlated with time information. This can be seen by plotting `classe` vs `raw_timestamp_part_1`

```{r timecorr, dependson='loaddata', cache=TRUE}
h<- ggplot(data=training,aes(x=raw_timestamp_part_1,y=classe,colour=classe)) + geom_point()
print(h + scale_x_continuous(limits=c(1323084231,1323084370)))
```

A model that incorporates time information as a predictor will not be useful when applied to new data 
The aim is to predict based on the sensor data. Will omit subject name, time, and window information from the training dataset, As these would not be useful predictors for measurements taken at later dates.

Also, there are a lot of missing values for a number of variables. These could be imputed, but for some of these columns only about 1.5% of the measurements are present. Will omit these variables also.

Finally, will partition the "training" data into training and validation sets. The validation set will be used to assess each model, and also to train the ensemble.

```{r tidy, dependson='loaddata', cache=TRUE}
na_list<-sapply(training,function(x) { sum(is.na(x))})
good<- na_list < 1900
nona<-training[good]
# remove name, time, window info
slimmed<-nona[,-(1:7)]
#str(slimmed)

# create training and validation partitions from original "training" set
inTrain<-createDataPartition(slimmed$classe,p=0.8,list=FALSE)
slimtr<-slimmed[inTrain,]
slimval<-slimmed[-inTrain,]
```

# Exploratory Analysis

A plot of the correlation between predictor variables is shown below.

```{r correlation,dependson='tidy',cache=TRUE}

thecorr<-cor(slimmed[-53])
thecorr[lower.tri(thecorr)]<- NA
mcm<-melt(thecorr,na.rm=TRUE)
head(mcm)
corrplot<-ggplot(data=mcm,aes(x=Var1,y=Var2,fill=value)) + geom_tile(colour='white') +
 scale_fill_gradientn(colours=c('blue','green')) +theme(axis.text.x = element_text(angle = 90, hjust = 1))
print(corrplot)
# h=ggplot(data=thecorr,)
```

There are a few patterns between off diagonal elements, indicating correlation between predictors, For example, the total accelerationof the dumbell and the x and y components of the dumbell gyro measurements are strongly (anti) correlated with the y and z components of the forearm gyro measurements. This suggests that a Principal Components Analysis (PCA) may be able to reduce the number of variables that need be considered while retaining the observed variance.

A pairs plot of the first 7 variables is shown below.

```{r pairplot,dependson='tidy',cache=TRUE,fig.width=8, fig.height=8,dpi=144}
h<-ggpairs(data=slimtr,columns=1:7,mapping=ggplot2::aes(colour = classe,alpha=0.02))
print(h)
```

# Model Selection 

## Linear Discriminant analysis.

This is like logistic regression for classification. Chapter 4 of ESL is good. All predictor variables are used. K-fold cross validation is used with 10 folds, repeated 3 times. Predictor variables are centred and scaled before the model is trained. This preprocessing is carried out in caret's `train` method, so that the data transformations are computed seperately for each fold, using only the training data in that fold. This lda implementation has no tuning parameters.

```{r lda,cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 
ldafit<-train(classe ~ . , data=slimtr, method='lda',trainControl=tr_c, preProcess=c('center','scale'))
endtime<-Sys.time()
endtime - starttime
```


### Model Assessment

Confusion matrix, error rates, accuracy metrics, etc. cross validation stuff. 

```{r ldastuff,cache=TRUE, dependson='lda'}

ldafit$results
# ldafit$finalModel

# confusionMatrix
ldapred<-predict(ldafit,newdata=slimval)
confusionMatrix(data=ldapred,reference=slimval$classe)

```
Accuracy isnt great. kappa isn't bad, but will try something else.

## Random Forest

Will use the random forest implementation from the ranger package. Random forests generate an ensemble of decision trees. Each tree is trained on a bootstrap resample of the data, and only a random subset of the predictor variables are considered at each split in the tree.

Number of variables to sample for each branch is governed by the `mtry` tuning parameter. The [Elements of Statistical Learining](https://statweb.stanford.edu/~tibs/ElemStatLearn/) recommends using the square root of the number of predictor variables as the guideline for mtry. There are 53 predictors, which would suggest an mtry=7-8. Will start with a coarse grid around mtry=10, and then refine it.

For cross validtion, will use k-fold cross validation with 10 folds repeated 3 times. Random forests don't neccesarily need to be cross-validated, this will be discuessed below.

```{r rangerInitial, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 

model_rangerInitial<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry=c(5,10,15)),importance='impurity')
endtime<-Sys.time()
endtime - starttime
```

```{r rangerInitialstuff,cache=TRUE,dependson='rangerInitial' }
model_rangerInitial

```

This performs pretty well. Model accuracy and $\kappa$ are both greater than 99%.  The optimal value of `mtry` was found to be 10, so will look for an optimal value close to that.

```{r rangerTuned, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 

# 20 fold cross validation, repeated 5 times
# can also select 'oob' for out of bag
# model_ranger_nopca<-train(classe~.,data=slim2,method='ranger',trControl=tr_c)
model_rangerTuned<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry= c(6,8,9,10,11,12)),importance='impurity')
endtime<-Sys.time()
endtime - starttime
```

```{r rangertunedplot,cache=TRUE,dependson='rangerTuned'}
ggplot(model_rangerTuned)
```

The optimal alue of mtry is found to be 9.

###  Model Assessment


break this chunk up
```{r rangerTunedstuff,cache=TRUE,dependson='rangerTuned' }
# plot accuracy vs mtry

# model details
model_rangerTuned
model_rangerTuned$finalModel

# plot variable importance
varimps<-varImp(model_rangerTuned)
imps<-data.frame(var=row.names(varimps$importance),importance=varimps$importance)
importance<-data.frame(var=row.names(imps$importance),Overall=imps$importance)
# keep only top 20 variables
imps<-imps[order(imps$Overall,decreasing=TRUE)[1:20],]
h<-ggplot(data=imps,aes(x=var,weight=Overall,fill=Overall)) + geom_bar() + 
scale_x_discrete(limits=imps$var) + scale_fill_gradientn(colors=c('purple','red')) + labs(y="relative importance (%)",x="variable") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) + guides(fill=FALSE)
print(h)

# Confusion matrices
# this one is generated from the training set using out-of-bag samples
model_rangerTuned$finalModel$confusion.matrix

# This is the confusion matrix generated by applying the final model to the vaidation set
rangerTunedpred<-predict(model_rangerTuned,newdata=slimval)
confusionMatrix(data=rangerTunedpred,reference=slimval$classe)
```

Random forests can estimate out of sample accuracy using out of bag samples. Each tree in the forest is trained using a bootstrapped resample of the training set, and so there are some observations in the training data that a given tree was not exposed to. These observations can be used to assess the tree's accuracy. The accuracy of the forest can be estimated by having it predict on the training set, but only obtaining predictions from trees which were not trained on the observation being predicted. This is the out-of-bag error rate.  The optimal model above has an out of bag error of 0.4%, or an accuracy of 99.6%. 

confustion matrix from oob

From the repeated k-fold cross-validation, model accuracy (at mtry=9) is estimated at 99.5% and $\kappa$ at 99.38%. These values are extremely good, which is nice. The accuracy is also consisted with the out-of-bag error rate.

Validation set
give similar performance, very good.
validation confusion matrix, very similar to the OOB one

Happy with this model
Variable importance



<!-- accuracy seems good, kappa very close to one, which is very good. (better than 0.4 is decent)
compares observed accuracy to expected, whichis the accuracy expected from just random guessing (based on confusion matrix)

Have a few ways of assessing error here. We did cross validation, which sould give us an idea of out of sample error. Random forests are a bootstrap method, and so we can get an out of bag error, which is "shown to be unbiased". Finally, we can partitioned of a validation set at the start, so we can use this. Generally cross validation is done when you want to use all data for training rather than reserving a subset for validation. Here we seem to have plenty of data, so no harm in saving some (indeed, our model performs very well). -->

## Dimension Reduction/compression

Random forests can run into difficulties when there are a few important predictors hidden amongst a large number of variables. (because we only sample a subset of the predictors at each branch, so there is a small probability of getting a good variable). Try to reduce the number of variables by pca, see if this improves things. 
First get an idea of how many components are needed to capture a certain amount of variance

```{r pcaCheck, cache=TRUE, dependson='tidy'}
set.seed(5074491)
preproc<-preProcess(slimtr[-53],method='pca',thresh=0.95)
preproc$numComp
preproc<-preProcess(slimtr[-53],method='pca',thresh=0.90)
preproc$numComp
preproc<-preProcess(slimtr[-53],method='pca',thresh=0.85)
preproc$numComp
```
Will aim for 95% variance, so will keep 26 components. We want the preprocessing to be carried out by the `train` function, so that it is done seperately using only the data in each fold. Preprocessing the dataset first and then training may introduce bias, as in that case the data in each fold will have been transformed based on the overall variance of the entire dataset - the transformation would depend on data not contained in the fold. It is also important to give the `train` method the number of PCA components to be used, rather than the relative amount of variance to be captured. It's possible that a constant variance threshold could result in different folds being transformed into a different number of PCA components. The ratio of variables sampled by the tree (mtry) to the total number of variables could then differ between folds. In extreme cases, a fold could contain fewer variables than the specified value of mtry, which would result in caret/ranger throwing errors. 


```{r rangerPCA, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats=3,allowParallel=TRUE,preProcOptions = list(pcaComp = 26)) 
model_rangerPCA<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid( mtry= c(2,3,4,5)),importance='impurity',preProcess='pca')
endtime<-Sys.time()
endtime - starttime
```

Took a (relatively) long time for only 4 mtry values and a reduced number of predictors.
rangerPCA stuff

```{r rangerPCAstuff,cache=TRUE,dependson='rangerPCA' }
ggplot(model_rangerPCA)
model_rangerPCA
model_rangerPCA$finalModel
#confusionMatrix
# confusionMatrix
model_rangerPCA$finalModel$confusion.matrix
rangerPCApred<-predict(model_rangerPCA,newdata=slimval,predict.all=TRUE)
dim(rangerPCApred)
confusionMatrix(data=rangerPCApred,reference=slimval$classe)
# summary(rangerPCApred)

#ROC curve
#result.roc <- roc(slimval$classe, rangerPCApred) # Draw ROC curve.
#plot(result.roc, print.thres="best", print.thres.best.method="closest.topleft")



# result.coords <- coords(result.roc, "best", best.method="closest.topleft", ret=c("threshold", "accuracy"))
# print(result.coords)#to get threshold and accuracy
```
doesnt perform super well. Also took a long time. Scalability. Will use the rangerTuned model above as our "final model".


# TODO


todo:

    * add roc curves - maybe not. Can get the votes from the model prediction, but need to convert them to "probability"
	  ** Not sure what pROC would want for this in a multiclass context
	* add more words
	* read esl bagging?


# Model Assessment
Random forests use a number (500) of different trees, each of which is derived from a bootstrap sample of the training data. An out of bag error estimate can be obtained by predicting observations in the training set, but only using the subset of trees in the forest in which the bootstrap sample did not contain the observation being predicted. In this way, the forest's prediction for the ith observation only depends on trees that were not trained to the ith observation. This is out of bag error.

Cross validation also gives us an error rate. The training sample is divided up into 10 folds. Each fold uses 9/10ths of the data for training and the remaining 1/10 for validation/testing. The cross validated error rate is the average of the error rates for each fold. For repeated cv's, the k-fold cross validation is repeated 5 times (in this case). the 10 folds are generated independantly for each repeat.

After the cross validation has been carried out, the final model is trained on the **full** training set using the best parameters found during the tuning process. 

Also,we have the validation set that we held back. This is data that has not been seen at all in the training process. This should be good for estimating error, would trust this the most. Generally use cross validation in cases where can't afford to hold data back for a validation set. Using cross validation here, but figured theres plenty of data, so might as well make a validation set anyway. Our models seem very good, so don't think anything was hurt by not training on our validation data.

What are kappa, etc? Kappa compares observed accuraccy to expected accuracy. Expected accuracy looks like correlation, except we are multiplying sums rather than summing products - which is equal to correlation if things are independant  $\langle a b \rangle = \langle a \rangle \langle b \rangle. So expected accuracy assumes no correlation, it's just random guessing.





<!-- ## Plots

exploratory - classe vs time correlation
correlation heatmap

models
tuning parameters

assessment
ROC curve

classProbs=TRUE in traincontrol
think ROC isn't going to work easily.


plot classe vs time
plot variable correlation

plot variable importance
plot tuning (x2)

Thus the AUC is equivalent to the probability that a randomly chosen member of class 1 will have a smaller estimated probability of belonging to class 0 than a randomly chosen member of class 0 -->


## Test predictions
load the data

```{r loadtest,cache=TRUE,dependson='rangerTuned'}
testing<-read.csv('./data/pml-testing.csv',stringsAsFactors=TRUE,na.strings = c("#DIV/0!","NA"))
dim(testing)
testing$cvtd_timestamp<-strptime(testing$cvtd_timestamp,format = "%d/%m/%Y %H:%M")

# there is no classe variable in the testing set
# classe is the last variable in "good", the list of variables without missing values from the "loaddata" chunk above
slimtest<-testing[ good[-length(good)]]
slimtest<-slimtest[,-(1:7)]
str(slimtest)

testPred<-predict(model_rangerTuned,newdata=slimtest)
results<-data.frame(case=seq_along(testPred),prediction=testPred)
#results

```
Test results are all good, according to the quiz. My prediction outputs are suppressed here as showing them may violate the coursera honour code.



<!-- ROC curve  -->
<!-- http://stackoverflow.com/questions/30366143/how-to-compute-roc-and-auc-under-roc-after-training-using-caret-in-r -->
