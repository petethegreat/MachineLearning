---
title: "Machine Learning Project"
output:
  html_document:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---


load data

```{r}
training<-read.csv('./data/pml-training.csv',stringsAsFactors=TRUE,na.strings = c("#DIV/0!","NA"))
dim(training)
str(training,list.len=ncol(training))
# training$cvtd_time<-strptime(training$cvtd_timestamp,format = "%d/%m/%Y %H:%M",list=FALSE)
```

## tidying
write a function to do this, as we will need to tidy the test data also
cvtd timestamp -> to date
 $ max_roll_belt 
  max_picth_belt 
  min_roll_belt   
  min_pitch_belt
amplitude_roll_belt   
A bunch of others

set things to missing, impute the missing values

Then procede

```{r}
na_list<-sapply(training,function(x) { sum(is.na(x))})
good<- na_list < 1900
slimmed<-training[good]
str(slimmed)
```


```{r}
library(ggplot2)
library(caret)
```

# glm

We'll try a glm (binomial) model (logit). Maybe this does a bunch of models?
make sure we do the preprocessing within the `train` statement, or else it messes up cross validation (We should split the data into folds and conduct PCA within each fold, rather than transforming via PCA and then splitting). 


## basic tree
get rid of cvtd timestamp first
add cross validaation to this

```{r]}
slim2<-slimmed[-5]
treefit<-train(classe ~ . , data=slim2, method='rpart')
treefit
treefit$finalModel
```
<!-- library(devtools) -->
<!-- install_bitbucket("mkuhn/parallelRandomForest", ref="parallelRandomForest") -->

```{r}
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.9))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
```




