---
title: "Activity classification from wearable device measurements"
output:
  html_document:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---

===============================

# Introduction

Data comes from XX. Aim is to predict whether subject is performing a weight lifting excercise correctly, or making one of several common mistakes.

Data will first be partitioned and tidied

Exploratory
 - correlation in time
 - correlation heatmap (after tidying)

 preprocessing - get an idea of how many components to use in PCA 
 should we use pca?


# Data Partitioning

First load required R libraries and set chunk options

load caret and ggplot2

```{r packages, cache=FALSE,message=FALSE,warning=FALSE}
library(knitr)
opts_chunk$set(fig.width=6, fig.height=6,dpi=108)
require(ggplot2)
require(dplyr)
require(caret)
require(ranger)
require(MASS)
require(reshape2)
```
Load data. The `cvtd_time` variable is converted to a posixlt class.

```{r loaddata,cache=TRUE}
training<-read.csv('./data/pml-training.csv',stringsAsFactors=TRUE,na.strings = c("#DIV/0!","NA"))
dim(training)
training$cvtd_timestamp<-strptime(training$cvtd_timestamp,format = "%d/%m/%Y %H:%M")
```

The activity classification is specified in the classe variable. In the training dataset, this is highly correlated with time information:

classe is correlated with time:

```{r timecorr, dependson='loaddata', cache=TRUE}
h<- ggplot(data=training,aes(x=raw_timestamp_part_1,y=classe,colour=classe)) + geom_point()
print(h + scale_x_continuous(limits=c(1323084231,1323084370)))
```
The aim is to predict based on the sensor data. Will omit subject name, time, and window information from the training dataset, As these would not be useful predictors for measurements taken at later dates.

Also, there are a lot of missing values for a number of variables. These could be imputed, but for some of these columns only about 1.5% of the measurements are present. Will omit these variables also.

Finally, will partition the "training" data into training and validation sets. The validation set will be used to assess each model, and also to train the ensemble.

```{r tidy, dependson='loaddata', cache=TRUE}
na_list<-sapply(training,function(x) { sum(is.na(x))})
good<- na_list < 1900
nona<-training[good]
# remove name, time, window info
slimmed<-nona[,-(1:7)]
str(slimmed)
inTrain<-createDataPartition(slimmed$classe,p=0.8,list=FALSE)
slimtr<-slimmed[inTrain,]
slimval<-slimmed[-inTrain,]
```

# Exploratory
look at correlation between slimmed variables

```{r correlation,dependson='tidy',cache=TRUE}

thecorr<-cor(slimmed[-53])
thecorr[lower.tri(thecorr)]<- NA
mcm<-melt(thecorr,na.rm=TRUE)
head(mcm)
corrplot<-ggplot(data=mcm,aes(x=Var1,y=Var2,fill=value)) + geom_tile(colour='white') +
 scale_fill_gradientn(colours=c('blue','green')) +theme(axis.text.x = element_text(angle = 90, hjust = 1))
print(corrplot)
# h=ggplot(data=thecorr,)

```

# Linear Discriminant analysis.

This is like logistic regression for classification. Chapter 4 of ESL is good. All predictor variables are used. K-fold cross validation is used with 10 folds, repeated 3 times. Preditor variables are centred and scaled before the model is trained. This preprocessing is carried out in caret's `train` method, so that the data transformations are computed seperately for each fold, using only the training data in that fold. 

```{r lda,cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 
ldafit<-train(classe ~ . , data=slimtr, method='lda',trainControl=tr_c, preProcess=c('center','scale'))
endtime<-Sys.time()
endtime - starttime
```

## results
Confusion matrix, error rates, accuracy metrics, etc. cross validation stuff. 

```{r ldastuff,cache=TRUE, dependson='lda'}

ldafit
ldafit$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varImp(ldafit)
# confusionMatrix
ldapred<-predict(ldafit,newdata=slimval)
confusionMatrix(data=ldapred,reference=slimval$classe)

```

# Random Forest

Use the random forest implementation from the ranger package
Use k-fold cross validation with 3 repeats. mtry parameter is the number of predictor variables sampled at each branch of the tree. 


```{r ranger2, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 

# 20 fold cross validation, repeated 5 times
# can also select 'oob' for out of bag
# model_ranger_nopca<-train(classe~.,data=slim2,method='ranger',trControl=tr_c)
model_ranger2<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry=c(5,10,15)),importance='impurity')
endtime<-Sys.time()
endtime - starttime
```

ranger2 stuff

```{r ranger2stuff,cache=TRUE,dependson='ranger2' }
ggplot(model_ranger2)
model_ranger2
model_ranger2$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varImp(model_ranger2)
# confusionMatrix
ranger2pred<-predict(model_ranger2,newdata=slimval)
confusionMatrix(data=ranger2pred,reference=slimval$classe)

```

This performs pretty well. Optimal value of mtry was found to be 10. Will try a few more values around here

```{r ranger2a, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 

# 20 fold cross validation, repeated 5 times
# can also select 'oob' for out of bag
# model_ranger_nopca<-train(classe~.,data=slim2,method='ranger',trControl=tr_c)
model_ranger2a<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry= c(6,8,9,10,11,12)),importance='impurity')
endtime<-Sys.time()
endtime - starttime
```

any improvement?

```{r ranger2astuff,cache=TRUE,dependson='ranger2' }
ggplot(model_ranger2a)
model_ranger2a
model_ranger2a$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varImp(model_ranger2a)
# confusionMatrix
ranger2apred<-predict(model_ranger2a,newdata=slimval)
confusionMatrix(data=ranger2apred,reference=slimval$classe)

```

try pca

```{r ranger3, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats=3,allowParallel=TRUE,preProcOptions = list(pcaComp = 25)) 
model_ranger3<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid( mtry= c(3,4,5,6)),importance='impurity',preProcess='pca')
endtime<-Sys.time()
endtime - starttime
```

ranger3 stuff

```{r ranger3stuff,cache=TRUE,dependson='ranger3' }
library(ROCR)
ggplot(model_ranger3)
model_ranger3
model_ranger3$finalModel
#confusionMatrix
# confusionMatrix
model_ranger3$finalModel$confusion.matrix
ranger3pred<-predict(model_ranger3,newdata=slimval,predict.all=TRUE)
dim(ranger3pred)
confusionMatrix(data=ranger3pred,reference=slimval$classe)
# summary(ranger3pred)

#ROC curve
#result.roc <- roc(slimval$classe, ranger3pred) # Draw ROC curve.
#plot(result.roc, print.thres="best", print.thres.best.method="closest.topleft")



# result.coords <- coords(result.roc, "best", best.method="closest.topleft", ret=c("threshold", "accuracy"))
# print(result.coords)#to get threshold and accuracy
```

todo:
add the pca components vs variance chunk
add roc curves
add a couple more plots and some words
read esl bagging/cross validation
maybe switch accuracy metrics to oob, or cross validation method to oob


# Model Assessment
Random forests use a number (500) of different trees, each of which is derived from a bootstrap sample of the training data. An out of bag error estimate can be obtained by predicting observations in the training set, but only using the subset of trees in the forest in which the bootstrap sample did not contain the observation being predicted. In this way, the forest's prediction for the ith observation only depends on trees that were not trained to the ith observation. This is out of bag error.

Cross validation also gives us an error rate. The training sample is divided up into 10 folds. Each fold uses 9/10ths of the data for training and the remaining 1/10 for validation/testing. The cross validated error rate is the average of the error rates for each fold. For repeated cv's, the k-fold cross validation is repeated 5 times (in this case). the 10 folds are generated independantly for each repeat.

After the cross validation has been carried out, the final model is trained on the **full** training set using the best parameters found during the tuning process. 

Also,we have the validation set that we held back. This is data that has not been seen at all in the training process. This should be good for estimating error, would trust this the most. Generally use cross validation in cases where can't afford to hold data back for a validation set. Using cross validation here, but figured theres plenty of data, so might as well make a validation set anyway. Our models seem very good, so don't think anything was hurt by not training on our validation data.

What are kappa, etc?



## Plots

exploratory - classe vs time correlation
correlation heatmap

models
tuning parameters

assessment
ROC curve




<!-- ROC curve  -->
<!-- http://stackoverflow.com/questions/30366143/how-to-compute-roc-and-auc-under-roc-after-training-using-caret-in-r -->
