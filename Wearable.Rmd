---
title: "Activity classification from wearable device measurements"
output:
  html_document:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---

===============================

# Introduction

Data comes from XX. Aim is to predict whether subject is performing a weight lifting excercise correctly, or making one of several common mistakes.

Data will first be partitioned and tidied

Exploratory
 - correlation in time
 - correlation heatmap (after tidying)

 preprocessing - get an idea of how many components to use in PCA 
 should we use pca?


# Data Partitioning

First load required R libraries and set chunk options

load caret and ggplot2

```{r packages, cache=FALSE,message=FALSE,warning=FALSE}
library(knitr)
opts_chunk$set(fig.width=6, fig.height=6,dpi=108)
library(ggplot2)
library(dplyr)
library(caret)
library(ranger)
library(MASS)
library(reshape2)
```
Load data. The `cvtd_time` variable is converted to a posixlt class.

```{r loaddata,cache=TRUE}
training<-read.csv('./data/pml-training.csv',stringsAsFactors=TRUE,na.strings = c("#DIV/0!","NA"))
dim(training)
training$cvtd_timestamp<-strptime(training$cvtd_timestamp,format = "%d/%m/%Y %H:%M")
```

The activity classification is specified in the classe variable. In the training dataset, this is highly correlated with time information:

classe is correlated with time:

```{r timecorr, dependson='loaddata', cache=TRUE}
h<- ggplot(data=training,aes(x=raw_timestamp_part_1,y=classe,colour=classe)) + geom_point()
print(h + scale_x_continuous(limits=c(1323084231,1323084370)))
```
The aim is to predict based on the sensor data. Will omit subject name, time, and window information from the training dataset, As these would not be useful predictors for measurements taken at later dates.

Also, there are a lot of missing values for a number of variables. These could be imputed, but for some of these columns only about 1.5% of the measurements are present. Will omit these variables also.

Finally, will partition the "training" data into training and validation sets. The validation set will be used to assess each model, and also to train the ensemble.

```{r tidy, dependson='loaddata', cache=TRUE}
na_list<-sapply(training,function(x) { sum(is.na(x))})
good<- na_list < 1900
nona<-training[good]
# remove name, time, window info
slimmed<-nona[,-(1:7)]
str(slimmed)
inTrain<-createDataPartition(slimmed$classe,p=0.8,list=FALSE)
slimtr<-slimmed[inTrain,]
slimval<-slimmed[-inTrain,]
```

# Exploratory
look at correlation between slimmed variables

```{r correlation,dependson='tidy',cache=TRUE}

thecorr<-cor(slimmed[-53])
thecorr[lower.tri(thecorr)]<- NA
mcm<-melt(thecorr,na.rm=TRUE)
head(mcm)
corrplot<-ggplot(data=mcm,aes(x=Var1,y=Var2,fill=value)) + geom_tile(colour='white') +
 scale_fill_gradientn(colours=c('blue','green')) +theme(axis.text.x = element_text(angle = 90, hjust = 1))
print(corrplot)
# h=ggplot(data=thecorr,)

```

# Linear Discriminant analysis.

This is like logistic regression for classification. Chapter 4 of ESL is good. All predictor variables are used. K-fold cross validation is used with 10 folds, repeated 3 times. Preditor variables are centred and scaled before the model is trained. This preprocessing is carried out in caret's `train` method, so that the data transformations are computed seperately for each fold, using only the training data in that fold. 

```{r lda,cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 
ldafit<-train(classe ~ . , data=slimtr, method='lda',trainControl=tr_c, preProcess=c('center','scale'))
endtime<-Sys.time()
endtime - starttime
```

could do a gbm if we really wanted
## results

Confusion matrix, error rates, accuracy metrics, etc. cross validation stuff. 

```{r ldastuff,cache=TRUE, dependson='lda'}

ldafit
ldafit$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varImp(ldafit)
# confusionMatrix
ldapred<-predict(ldafit,newdata=slimval)
confusionMatrix(data=ldapred,reference=slimval$classe)

```

# Random Forest

Use the random forest implementation from the ranger package
Use k-fold cross validation with 3 repeats. mtry parameter is the number of predictor variables sampled at each branch of the tree. 


```{r rangerInitial, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 

# 20 fold cross validation, repeated 5 times
# can also select 'oob' for out of bag
# model_ranger_nopca<-train(classe~.,data=slim2,method='ranger',trControl=tr_c)
model_rangerInitial<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry=c(5,10,15)),importance='impurity')
endtime<-Sys.time()
endtime - starttime
```

rangerInitial stuff

```{r rangerInitialstuff,cache=TRUE,dependson='rangerInitial' }
ggplot(model_rangerInitial)
model_rangerInitial
model_rangerInitial$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varImp(model_rangerInitial)
# confusionMatrix
rangerInitialpred<-predict(model_rangerInitial,newdata=slimval)
confusionMatrix(data=rangerInitialpred,reference=slimval$classe)

```

This performs pretty well. Optimal value of mtry was found to be 10. Will try a few more values around here

```{r rangerTuned, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 

# 20 fold cross validation, repeated 5 times
# can also select 'oob' for out of bag
# model_ranger_nopca<-train(classe~.,data=slim2,method='ranger',trControl=tr_c)
model_rangerTuned<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry= c(6,8,9,10,11,12)),importance='impurity')
endtime<-Sys.time()
endtime - starttime
```

any improvement?

```{r rangerTunedstuff,cache=TRUE,dependson='rangerTuned' }
ggplot(model_rangerTuned)
model_rangerTuned
model_rangerTuned$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varimps<-varImp(model_rangerTuned)
imps<-data.frame(var=row.names(varimps$importance),importance=varimps$importance)
importance<-data.frame(var=row.names(imps$importance),Overall=imps$importance)
# keep only top 20 variables
imps<-imps[order(imps$Overall,decreasing=TRUE)[1:20],]
h<-ggplot(data=imps,aes(x=var,weight=Overall,fill=Overall)) + geom_bar() + 
scale_x_discrete(limits=imps$var) + scale_fill_gradientn(colors=c('purple','red')) + labs(y="relative importance (%)",x="variable") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) + guides(fill=FALSE)
print(h)
# confusionMatrix
rangerTunedpred<-predict(model_rangerTuned,newdata=slimval)
confusionMatrix(data=rangerTunedpred,reference=slimval$classe)

```

try pca

```{r rangerPCA, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats=3,allowParallel=TRUE,preProcOptions = list(pcaComp = 25)) 
model_rangerPCA<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid( mtry= c(2,3,4,5)),importance='impurity',preProcess='pca')
endtime<-Sys.time()
endtime - starttime
```

rangerPCA stuff

```{r rangerPCAstuff,cache=TRUE,dependson='rangerPCA' }
ggplot(model_rangerPCA)
model_rangerPCA
model_rangerPCA$finalModel
#confusionMatrix
# confusionMatrix
model_rangerPCA$finalModel$confusion.matrix
rangerPCApred<-predict(model_rangerPCA,newdata=slimval,predict.all=TRUE)
dim(rangerPCApred)
confusionMatrix(data=rangerPCApred,reference=slimval$classe)
# summary(rangerPCApred)

#ROC curve
#result.roc <- roc(slimval$classe, rangerPCApred) # Draw ROC curve.
#plot(result.roc, print.thres="best", print.thres.best.method="closest.topleft")



# result.coords <- coords(result.roc, "best", best.method="closest.topleft", ret=c("threshold", "accuracy"))
# print(result.coords)#to get threshold and accuracy
```

todo:
add the pca components vs variance chunk
add roc curves
add a couple more plots and some words
read esl bagging/cross validation
maybe switch accuracy metrics to oob, or cross validation method to oob


# Model Assessment
Random forests use a number (500) of different trees, each of which is derived from a bootstrap sample of the training data. An out of bag error estimate can be obtained by predicting observations in the training set, but only using the subset of trees in the forest in which the bootstrap sample did not contain the observation being predicted. In this way, the forest's prediction for the ith observation only depends on trees that were not trained to the ith observation. This is out of bag error.

Cross validation also gives us an error rate. The training sample is divided up into 10 folds. Each fold uses 9/10ths of the data for training and the remaining 1/10 for validation/testing. The cross validated error rate is the average of the error rates for each fold. For repeated cv's, the k-fold cross validation is repeated 5 times (in this case). the 10 folds are generated independantly for each repeat.

After the cross validation has been carried out, the final model is trained on the **full** training set using the best parameters found during the tuning process. 

Also,we have the validation set that we held back. This is data that has not been seen at all in the training process. This should be good for estimating error, would trust this the most. Generally use cross validation in cases where can't afford to hold data back for a validation set. Using cross validation here, but figured theres plenty of data, so might as well make a validation set anyway. Our models seem very good, so don't think anything was hurt by not training on our validation data.

What are kappa, etc? Kappa compares observed accuraccy to expected accuracy. Expected accuracy looks like correlation, except we are multiplying sums rather than summing products - which is equal to correlation if things are independant  $\langle a b \rangle = \langle a \rangle \langle b \rangle. So expected accuracy assumes no correlation, it's just random guessing.





## Plots

exploratory - classe vs time correlation
correlation heatmap

models
tuning parameters

assessment
ROC curve

classProbs=TRUE in traincontrol
think ROC isn't going to work easily. 


plot classe vs time
plot variable correlation

plot variable importance
plot tuning (x2)

Thus the AUC is equivalent to the probability that a randomly chosen member of class 1 will have a smaller estimated probability of belonging to class 0 than a randomly chosen member of class 0


## Test predictions
load the data

```{r loadtest,cache=TRUE,dependson='rangerTuned'}
testing<-read.csv('./data/pml-testing.csv',stringsAsFactors=TRUE,na.strings = c("#DIV/0!","NA"))
dim(testing)
testing$cvtd_timestamp<-strptime(testing$cvtd_timestamp,format = "%d/%m/%Y %H:%M")

# there is no classe variable in the testing set
# classe is the last variable in "good", the list of variables without missing values from the "loaddata" chunk above
slimtest<-testing[ good[-length(good)]]
slimtest<-slimtest[,-(1:7)]
str(slimtest)

testPred<-predict(model_rangerTuned,newdata=slimtest)
results<-data.frame(case=seq_along(testPred),prediction=testPred)
#results

```
Test results are all good, according to the quiz. My prediction outputs are suppressed here as showing them may violate the coursera honour code.



<!-- ROC curve  -->
<!-- http://stackoverflow.com/questions/30366143/how-to-compute-roc-and-auc-under-roc-after-training-using-caret-in-r -->
