---
title: "Activity classification from wearable device measurements"
output:
  html_document:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---

===============================

# Introduction

Data comes from XX. Aim is to predict whether subject is performing a weight lifting excercise correctly, or making one of several common mistakes.

Data will first be partitioned and tidied

Exploratory
 - correlation in time
 - correlation heatmap (after tidying)

 preprocessing - get an idea of how many components to use in PCA 
 should we use pca?


# Data Partitioning

First load required R libraries and set chunk options

load caret and ggplot2

```{r packages, cache=FALSE,message=FALSE,warning=FALSE}
library(knitr)
opts_chunk$set(fig.width=6, fig.height=6,dpi=108)
library(ggplot2)
library(dplyr)
library(caret)
library(ranger)
library(MASS)
library(reshape2)
library(GGally)
```
Load data. 

```{r loaddata,cache=TRUE}
training<-read.csv('./data/pml-training.csv',stringsAsFactors=TRUE,na.strings = c("#DIV/0!","NA"))
dim(training)
# convert cvtd_timestamp to posixlt class
training$cvtd_timestamp<-strptime(training$cvtd_timestamp,format = "%d/%m/%Y %H:%M")
```

The activity classification is specified in the `classe` variable. In the training dataset, this is highly correlated with time information. This can be seen by plotting `classe` vs `raw_timestamp_part_1`

```{r timecorr, dependson='loaddata', cache=TRUE}
h<- ggplot(data=training,aes(x=raw_timestamp_part_1,y=classe,colour=classe)) + geom_point()
print(h + scale_x_continuous(limits=c(1323084231,1323084370)))
```

A model that incorporates time information as a predictor will not be useful when applied to new data 
The aim is to predict based on the sensor data. Will omit subject name, time, and window information from the training dataset, As these would not be useful predictors for measurements taken at later dates.

Also, there are a lot of missing values for a number of variables. These could be imputed, but for some of these columns only about 1.5% of the measurements are present. Will omit these variables also.

Finally, will partition the "training" data into training and validation sets. The validation set will be used to assess each model, and also to train the ensemble.

```{r tidy, dependson='loaddata', cache=TRUE}
na_list<-sapply(training,function(x) { sum(is.na(x))})
good<- na_list < 1900
nona<-training[good]
# remove name, time, window info
slimmed<-nona[,-(1:7)]
str(slimmed)
inTrain<-createDataPartition(slimmed$classe,p=0.8,list=FALSE)
slimtr<-slimmed[inTrain,]
slimval<-slimmed[-inTrain,]
```

# Exploratory

A plot of the correlation between predictor variables is shown below

```{r correlation,dependson='tidy',cache=TRUE}

thecorr<-cor(slimmed[-53])
thecorr[lower.tri(thecorr)]<- NA
mcm<-melt(thecorr,na.rm=TRUE)
head(mcm)
corrplot<-ggplot(data=mcm,aes(x=Var1,y=Var2,fill=value)) + geom_tile(colour='white') +
 scale_fill_gradientn(colours=c('blue','green')) +theme(axis.text.x = element_text(angle = 90, hjust = 1))
print(corrplot)
# h=ggplot(data=thecorr,)
```

There are a few patterns between off diagonal elements, indicating correlation between predictors, For example, the total accelerationof the dumbell and the x and y components of the dumbell gyro measurements are strongly (anti) correlated with the y and z components of the forearm gyro measurements. This suggests that a Principal Components Analysis (PCA) may be able to reduce the number of variables that need be considered while retaining the observed variance.

A pairs plot of the first 7 variables is shown below.

```{r pairplot,dependson='tidy',cache=TRUE,fig.width=8, fig.height=8,dpi=144}
h<-ggpairs(data=slimtr,columns=1:7,mapping=ggplot2::aes(colour = classe,alpha=0.02))
print(h)
```



# Model Selection

## Cross Validation
caret has a few methods for cross validation. Will use repeatedcv, with number = 10 and repeats =3. This carries out 3 iterations of 10-fold cross validation. For each iteration, the training data is partitioned into 10 folds. 

## Linear Discriminant analysis.

This is like logistic regression for classification. Chapter 4 of ESL is good. All predictor variables are used. K-fold cross validation is used with 10 folds, repeated 3 times. Preditor variables are centred and scaled before the model is trained. This preprocessing is carried out in caret's `train` method, so that the data transformations are computed seperately for each fold, using only the training data in that fold. This lda implementation has no tuning parameters.

```{r lda,cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 
ldafit<-train(classe ~ . , data=slimtr, method='lda',trainControl=tr_c, preProcess=c('center','scale'))
endtime<-Sys.time()
endtime - starttime
```

### Model Assessment

Confusion matrix, error rates, accuracy metrics, etc. cross validation stuff. 

```{r ldastuff,cache=TRUE, dependson='lda'}

ldafit
ldafit$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varImp(ldafit)
# confusionMatrix
ldapred<-predict(ldafit,newdata=slimval)
confusionMatrix(data=ldapred,reference=slimval$classe)

```
Accuracy isnt great. kappa isn't bad, but will try something else.

## Random Forest

Use the random forest implementation from the ranger package.
Random Forests are create a bootstrap ensemble of decision trees. 

Number of variables to sample for each branch is governed by the mtry tuning parameter
ESL recommends sqrt of number of variables for classification. There are 53 predictors, so suggests an mtry=7-8. Will try a coarse grid to start, then refine it

```{r rangerInitial, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 

# 20 fold cross validation, repeated 5 times
# can also select 'oob' for out of bag
# model_ranger_nopca<-train(classe~.,data=slim2,method='ranger',trControl=tr_c)
model_rangerInitial<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry=c(5,10,15)),importance='impurity')
endtime<-Sys.time()
endtime - starttime
```

rangerInitial stuff

```{r rangerInitialstuff,cache=TRUE,dependson='rangerInitial' }
ggplot(model_rangerInitial)
model_rangerInitial
model_rangerInitial$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varImp(model_rangerInitial)
# confusionMatrix
rangerInitialpred<-predict(model_rangerInitial,newdata=slimval)
confusionMatrix(data=rangerInitialpred,reference=slimval$classe)

```
This performs pretty well. Optimal value of mtry was found to be 10. Will try a few more values around here

```{r rangerTuned, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats =3,allowParallel=TRUE) 

# 20 fold cross validation, repeated 5 times
# can also select 'oob' for out of bag
# model_ranger_nopca<-train(classe~.,data=slim2,method='ranger',trControl=tr_c)
model_rangerTuned<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry= c(6,8,9,10,11,12)),importance='impurity')
endtime<-Sys.time()
endtime - starttime
```

###  Model Assessment

```{r rangerTunedstuff,cache=TRUE,dependson='rangerTuned' }
ggplot(model_rangerTuned)
model_rangerTuned
model_rangerTuned$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varimps<-varImp(model_rangerTuned)
imps<-data.frame(var=row.names(varimps$importance),importance=varimps$importance)
importance<-data.frame(var=row.names(imps$importance),Overall=imps$importance)
# keep only top 20 variables
imps<-imps[order(imps$Overall,decreasing=TRUE)[1:20],]
h<-ggplot(data=imps,aes(x=var,weight=Overall,fill=Overall)) + geom_bar() + 
scale_x_discrete(limits=imps$var) + scale_fill_gradientn(colors=c('purple','red')) + labs(y="relative importance (%)",x="variable") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) + guides(fill=FALSE)
print(h)
# confusionMatrix
rangerTunedpred<-predict(model_rangerTuned,newdata=slimval)
confusionMatrix(data=rangerTunedpred,reference=slimval$classe)

```

optimal value seems to be around 9. 

accuracy seems good, kappa very close to one, which is very good. (better than 0.4 is decent)
compares observed accuracy to expected, whichis the accuracy expected from just random guessing (based on confusion matrix)

Have a few ways of assessing error here. We did cross validation, which sould give us an idea of out of sample error. Random forests are a bootstrap method, and so we can get an out of bag error, which is "shown to be unbiased". Finally, we can partitioned of a validation set at the start, so we can use this. Generally cross validation is done when you want to use all data for training rather than reserving a subset for validation. Here we seem to have plenty of data, so no harm in saving some (indeed, our model performs very well).

#### Cross validation

can look at kappa and the confusion matrix

#### out of bag

out of bag accuracy is very good, agrees well with that from cross validation

#### validation

accuracy/kapp both agree with previous values
confusion matrix looks great


## Dimension Reduction/compression

Random forests can run into difficulties when there are a few important predictors hidden amongst a large number of variables. (because we only sample a subset of the predictors at each branch, so there is a small probability of getting a good variable). Try to reduce the number of variables by pca, see if this improves things. 
First get an idea of how many components are needed to capture a certain amount of variance

```{r pcaCheck, cache=TRUE, dependson='tidy'}
set.seed(5074491)
preproc<-preProcess(slimtr[-53],method='pca',thresh=0.95)
preproc$numComp
preproc<-preProcess(slimtr[-53],method='pca',thresh=0.90)
preproc$numComp
preproc<-preProcess(slimtr[-53],method='pca',thresh=0.85)
preproc$numComp
```
Will aim for 95% variance, so will keep 26 components. We want the preprocessing to be carried out by the `train` function, so that it is done seperately using only the data in each fold. Preprocessing the dataset first and then training may introduce bias, as in that case the data in each fold will have been transformed based on the overall variance of the entire dataset - the transformation would depend on data not contained in the fold. It is also important to give the `train` method the number of PCA components to be used, rather than the relative amount of variance to be captured. It's possible that a constant variance threshold could result in different folds being transformed into a different number of PCA components. The ratio of variables sampled by the tree (mtry) to the total number of variables could then differ between folds. In extreme cases, a fold could contain fewer variables than the specified value of mtry, which would result in caret/ranger throwing errors. 


```{r rangerPCA, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats=3,allowParallel=TRUE,preProcOptions = list(pcaComp = 26)) 
model_rangerPCA<-train(classe ~ .  ,data=slimtr,method='ranger',trControl=tr_c,tuneGrid = expand.grid( mtry= c(2,3,4,5)),importance='impurity',preProcess='pca')
endtime<-Sys.time()
endtime - starttime
```

Took a (relatively) long time for only 4 mtry values and a reduced number of predictors.
rangerPCA stuff

```{r rangerPCAstuff,cache=TRUE,dependson='rangerPCA' }
ggplot(model_rangerPCA)
model_rangerPCA
model_rangerPCA$finalModel
#confusionMatrix
# confusionMatrix
model_rangerPCA$finalModel$confusion.matrix
rangerPCApred<-predict(model_rangerPCA,newdata=slimval,predict.all=TRUE)
dim(rangerPCApred)
confusionMatrix(data=rangerPCApred,reference=slimval$classe)
# summary(rangerPCApred)

#ROC curve
#result.roc <- roc(slimval$classe, rangerPCApred) # Draw ROC curve.
#plot(result.roc, print.thres="best", print.thres.best.method="closest.topleft")



# result.coords <- coords(result.roc, "best", best.method="closest.topleft", ret=c("threshold", "accuracy"))
# print(result.coords)#to get threshold and accuracy
```
doesnt perform super well. Also took a long time. Scalability. Will use the rangerTuned model above as our "final model".


# TODO


todo:

    * add roc curves - maybe not. Can get the votes from the model prediction, but need to convert them to "probability"
	  ** Not sure what pROC would want for this in a multiclass context
	* add more words
	* read esl bagging?


# Model Assessment
Random forests use a number (500) of different trees, each of which is derived from a bootstrap sample of the training data. An out of bag error estimate can be obtained by predicting observations in the training set, but only using the subset of trees in the forest in which the bootstrap sample did not contain the observation being predicted. In this way, the forest's prediction for the ith observation only depends on trees that were not trained to the ith observation. This is out of bag error.

Cross validation also gives us an error rate. The training sample is divided up into 10 folds. Each fold uses 9/10ths of the data for training and the remaining 1/10 for validation/testing. The cross validated error rate is the average of the error rates for each fold. For repeated cv's, the k-fold cross validation is repeated 5 times (in this case). the 10 folds are generated independantly for each repeat.

After the cross validation has been carried out, the final model is trained on the **full** training set using the best parameters found during the tuning process. 

Also,we have the validation set that we held back. This is data that has not been seen at all in the training process. This should be good for estimating error, would trust this the most. Generally use cross validation in cases where can't afford to hold data back for a validation set. Using cross validation here, but figured theres plenty of data, so might as well make a validation set anyway. Our models seem very good, so don't think anything was hurt by not training on our validation data.

What are kappa, etc? Kappa compares observed accuraccy to expected accuracy. Expected accuracy looks like correlation, except we are multiplying sums rather than summing products - which is equal to correlation if things are independant  $\langle a b \rangle = \langle a \rangle \langle b \rangle. So expected accuracy assumes no correlation, it's just random guessing.





<!-- ## Plots

exploratory - classe vs time correlation
correlation heatmap

models
tuning parameters

assessment
ROC curve

classProbs=TRUE in traincontrol
think ROC isn't going to work easily.


plot classe vs time
plot variable correlation

plot variable importance
plot tuning (x2)

Thus the AUC is equivalent to the probability that a randomly chosen member of class 1 will have a smaller estimated probability of belonging to class 0 than a randomly chosen member of class 0 -->


## Test predictions
load the data

```{r loadtest,cache=TRUE,dependson='rangerTuned'}
testing<-read.csv('./data/pml-testing.csv',stringsAsFactors=TRUE,na.strings = c("#DIV/0!","NA"))
dim(testing)
testing$cvtd_timestamp<-strptime(testing$cvtd_timestamp,format = "%d/%m/%Y %H:%M")

# there is no classe variable in the testing set
# classe is the last variable in "good", the list of variables without missing values from the "loaddata" chunk above
slimtest<-testing[ good[-length(good)]]
slimtest<-slimtest[,-(1:7)]
str(slimtest)

testPred<-predict(model_rangerTuned,newdata=slimtest)
results<-data.frame(case=seq_along(testPred),prediction=testPred)
#results

```
Test results are all good, according to the quiz. My prediction outputs are suppressed here as showing them may violate the coursera honour code.



<!-- ROC curve  -->
<!-- http://stackoverflow.com/questions/30366143/how-to-compute-roc-and-auc-under-roc-after-training-using-caret-in-r -->
