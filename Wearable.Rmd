---
title: "Machine Learning Project"
output:
  html_document:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---

===============================
load caret and ggplot2

```{r packages, cache=FALSE}
library(knitr)
opts_chunk$set(fig.width=8, fig.height=8,dpi=144)
require(ggplot2)
require(dplyr)
require(caret)
require(ranger)
```
load data

```{r loaddata,cache=TRUE}
training<-read.csv('./data/pml-training.csv',stringsAsFactors=TRUE,na.strings = c("#DIV/0!","NA"))
dim(training)
#str(training,list.len=ncol(training))
# training$cvtd_time<-strptime(training$cvtd_timestamp,format = "%d/%m/%Y %H:%M",list=FALSE)
```


## Timestamps
classe is correlated with time:

```{r timecorr, dependson='loaddata', cache=TRUE}
h<- ggplot(data=training,aes(x=raw_timestamp_part_1,y=classe,colour=classe)) + geom_point()
print(h + scale_x_continuous(limits=c(1323084231,1323084370)))
print(h + scale_x_continuous(limits=c(1323094900,1323095100)))
```

will want to strip irrelevant info, user, time, window stuff from training data.


## tidying
write a function to do this, as we will need to tidy the test data also
cvtd timestamp -> to date
 $ max_roll_belt 
  max_picth_belt 
  min_roll_belt   
  min_pitch_belt
amplitude_roll_belt   
A bunch of others

set things to missing, impute the missing values

Then procede

```{r tidy, dependson='loaddata', cache=TRUE}
na_list<-sapply(training,function(x) { sum(is.na(x))})
good<- na_list < 1900
nona<-training[good]
# remove name, time, window info
slimmed<-nona[,-(1:7)]
str(slimmed)
```



## some exploratory plots

# glm

We'll try a glm (binomial) model (logit). Maybe this does a bunch of models?
make sure we do the preprocessing within the `train` statement, or else it messes up cross validation (We should split the data into folds and conduct PCA within each fold, rather than transforming via PCA and then splitting). 




## basic tree
```{r rpart,cache=TRUE, dependson='tidy'}
treefit<-train(classe ~ . , data=slimmed, method='rpart')
treefit
treefit$finalModel
ggplot(treefit)
```
<!-- library(devtools) -->
<!-- install_bitbucket("mkuhn/parallelRandomForest", ref="parallelRandomForest") -->
## random forest
```{r ranger, cache=TRUE, dependson='tidy'}
set.seed(5074491)
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='cv',number=10,allowParallel=TRUE) 

# 20 fold cross validation, repeated 5 times
# can also select 'oob' for out of bag
# model_ranger_nopca<-train(classe~.,data=slim2,method='ranger',trControl=tr_c)
model_ranger<-train(classe ~ .  ,data=slimmed,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry= c(5,10,20)),importance='impurity')
ggplot(model_ranger)
model_ranger
model_ranger$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
```
Important variables

```{r varimportance,cache=TRUE,dependson='ranger'}
moose<-varImp(model_ranger)
mostimport <- order(moose$importance$Overall,decreasing=TRUE)
h<-ggpairs(data=slimmed,columns=mostimport[1:5],mapping=ggplot2::aes(colour = classe,alpha=0.1))
print(h)
```

A second random forest, this time using repeated cross validation, and maybe some more variable samplings

```{r ranger2, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats=3,allowParallel=TRUE) 

# 20 fold cross validation, repeated 5 times
# can also select 'oob' for out of bag
# model_ranger_nopca<-train(classe~.,data=slim2,method='ranger',trControl=tr_c)
model_ranger2<-train(classe ~ .  ,data=slimmed,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry= c(2,10,30)),importance='impurity')
endtime<-Sys.time()
endtime - starttime
```
ranger2 stuff

```{r ranger2stuff,cache=TRUE,dependson='ranger2' }
ggplot(model_ranger2)
model_ranger2
model_ranger2$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varImp(model_ranger2)
```

ranger3, random forest with pca

```{r ranger3, cache=TRUE, dependson='tidy'}
set.seed(5074491)
starttime<-Sys.time()
# tr_c<-trainControl(method='repeatedcv',number=20,repeats=5,allowParallel=TRUE)
tr_c<-trainControl(method='repeatedcv',number=10,repeats=3,allowParallel=TRUE,preProcOptions = list(thresh = 0.8)) 
model_ranger3<-train(classe ~ .  ,data=slimmed,method='ranger',trControl=tr_c,tuneGrid = expand.grid(mtry= c(2,4,8,10)),importance='impurity',preProcess='pca')
endtime<-Sys.time()
endtime - starttime
```

ranger3 stuff

```{r ranger3stuff,cache=TRUE,dependson='ranger3' }
ggplot(model_ranger3)
model_ranger3
model_ranger3$finalModel
# tr_c<-trainControl(method="cv",preProcOptions = list(thresh = 0.8))
# model_glm<-train(classe~.,data=slimmed,method='glm',family='binomial',train_control=tr_c,preProcess=c('pca','knnImpute'))
# summary(model_glm)
varImp(model_ranger3)
```
## Actually PCA
use some PCA preprocessing, maybe with another random forest

also look into svm

maybe one other model (gbm?) boosted decision tree?







